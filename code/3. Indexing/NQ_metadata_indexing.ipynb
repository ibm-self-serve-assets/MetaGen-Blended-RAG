{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NQ - Metadata Indexing\n",
    "This notebook demonstrates the process of indexing and evaluating metadata for the NQ (Natural Questions) dataset using Elasticsearch. The workflow includes:\n",
    "\n",
    "- **Data Preparation:** Loading and merging metadata from multiple sources.\n",
    "- **Elasticsearch Indexing:** Creating and configuring BM25 and KNN indices with custom analyzers and mappings.\n",
    "- **Embedding Generation:** Using Sentence Transformers to generate dense vector representations for KNN search.\n",
    "- **Indexing Pipeline:** Bulk indexing documents into Elasticsearch for both BM25 and KNN indices.\n",
    "\n",
    "Please follow the code cells and markdown explanations for a step-by-step guide through the indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(folder_name):\n",
    "    # Change the directory\n",
    "    os.chdir(folder_name)\n",
    "    # iterate through all file\n",
    "    file_path_list =[]\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        file_path = f\"{folder_name}/{file}\"\n",
    "        file_path_list.append(file_path)\n",
    "    return file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_all_files('../../output/nq/meta-data/')\n",
    "files1 = get_all_files('../../output/nq/meta-data-process/')\n",
    "files2 = get_all_files('../../output/nq/meta-data-lost/')\n",
    "dfs = []\n",
    "for file in files:\n",
    "    print(file)\n",
    "    df = pd.read_excel(file)  \n",
    "    print(\"length of file-----\",len(df))\n",
    "    dfs.append(df)\n",
    "print(\"New files-------\")\n",
    "for file in files1:\n",
    "    print(file)\n",
    "    df = pd.read_excel(file)\n",
    "    print(\"length of file-----\",len(df))  \n",
    "    dfs.append(df)\n",
    "\n",
    "for file in files2:\n",
    "    print(file)\n",
    "    df = pd.read_excel(file)\n",
    "    print(\"length of file-----\",len(df))  \n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all dataframes\n",
    "merged_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681468"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "      <th>keybert_title</th>\n",
       "      <th>yake_key_idea</th>\n",
       "      <th>extracted_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1370000</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>Also beginning in 2010, the American Animal Ho...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy bowl</td>\n",
       "      <td>Animal Hospital Association</td>\n",
       "      <td>2010, the American Animal Hospital Association...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1370001</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>A new element for 2011 was a parody of the pop...</td>\n",
       "      <td>{}</td>\n",
       "      <td>kiss cam</td>\n",
       "      <td>Kiss Cam</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1370002</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>Two other new elements were added in 2012 as w...</td>\n",
       "      <td>{}</td>\n",
       "      <td>cockatiel</td>\n",
       "      <td>elements were added</td>\n",
       "      <td>2012, Twitter, Jill Rappaport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1370003</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>The hamsters in the blimp and Meep the \"tweeti...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy cam</td>\n",
       "      <td>blimp and Meep</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1370004</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>For the 2014 edition of the Puppy Bowl, the te...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy bowl</td>\n",
       "      <td>Puppy Bowl parties</td>\n",
       "      <td>the Puppy Bowl, 2014, Michelle Obama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          _id       title                                               text  \\\n",
       "0  doc1370000  Puppy Bowl  Also beginning in 2010, the American Animal Ho...   \n",
       "1  doc1370001  Puppy Bowl  A new element for 2011 was a parody of the pop...   \n",
       "2  doc1370002  Puppy Bowl  Two other new elements were added in 2012 as w...   \n",
       "3  doc1370003  Puppy Bowl  The hamsters in the blimp and Meep the \"tweeti...   \n",
       "4  doc1370004  Puppy Bowl  For the 2014 edition of the Puppy Bowl, the te...   \n",
       "\n",
       "  metadata keybert_title                yake_key_idea  \\\n",
       "0       {}    puppy bowl  Animal Hospital Association   \n",
       "1       {}      kiss cam                     Kiss Cam   \n",
       "2       {}     cockatiel          elements were added   \n",
       "3       {}     puppy cam               blimp and Meep   \n",
       "4       {}    puppy bowl           Puppy Bowl parties   \n",
       "\n",
       "                                  extracted_entities  \n",
       "0  2010, the American Animal Hospital Association...  \n",
       "1                                               2011  \n",
       "2                      2012, Twitter, Jill Rappaport  \n",
       "3                                               2013  \n",
       "4               the Puppy Bowl, 2014, Michelle Obama  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'title', 'text', 'metadata', 'keybert_title', 'yake_key_idea',\n",
       "       'extracted_entities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "      <th>keybert_title</th>\n",
       "      <th>yake_key_idea</th>\n",
       "      <th>extracted_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1370000</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>Also beginning in 2010, the American Animal Ho...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy bowl</td>\n",
       "      <td>Animal Hospital Association</td>\n",
       "      <td>2010, the American Animal Hospital Association...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1370001</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>A new element for 2011 was a parody of the pop...</td>\n",
       "      <td>{}</td>\n",
       "      <td>kiss cam</td>\n",
       "      <td>Kiss Cam</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1370002</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>Two other new elements were added in 2012 as w...</td>\n",
       "      <td>{}</td>\n",
       "      <td>cockatiel</td>\n",
       "      <td>elements were added</td>\n",
       "      <td>2012, Twitter, Jill Rappaport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1370003</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>The hamsters in the blimp and Meep the \"tweeti...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy cam</td>\n",
       "      <td>blimp and Meep</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1370004</td>\n",
       "      <td>Puppy Bowl</td>\n",
       "      <td>For the 2014 edition of the Puppy Bowl, the te...</td>\n",
       "      <td>{}</td>\n",
       "      <td>puppy bowl</td>\n",
       "      <td>Puppy Bowl parties</td>\n",
       "      <td>the Puppy Bowl, 2014, Michelle Obama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          _id       title                                               text  \\\n",
       "0  doc1370000  Puppy Bowl  Also beginning in 2010, the American Animal Ho...   \n",
       "1  doc1370001  Puppy Bowl  A new element for 2011 was a parody of the pop...   \n",
       "2  doc1370002  Puppy Bowl  Two other new elements were added in 2012 as w...   \n",
       "3  doc1370003  Puppy Bowl  The hamsters in the blimp and Meep the \"tweeti...   \n",
       "4  doc1370004  Puppy Bowl  For the 2014 edition of the Puppy Bowl, the te...   \n",
       "\n",
       "  metadata keybert_title                yake_key_idea  \\\n",
       "0       {}    puppy bowl  Animal Hospital Association   \n",
       "1       {}      kiss cam                     Kiss Cam   \n",
       "2       {}     cockatiel          elements were added   \n",
       "3       {}     puppy cam               blimp and Meep   \n",
       "4       {}    puppy bowl           Puppy Bowl parties   \n",
       "\n",
       "                                  extracted_entities  \n",
       "0  2010, the American Animal Hospital Association...  \n",
       "1                                               2011  \n",
       "2                      2012, Twitter, Jill Rappaport  \n",
       "3                                               2013  \n",
       "4               the Puppy Bowl, 2014, Michelle Obama  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Path to your .env file\n",
    "env_path = \"../.env\"  # Change path if needed\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "# Access the environment variables\n",
    "ES_URL = os.getenv(\"ES_URL\")\n",
    "ES_USER = os.getenv(\"ES_USER\")\n",
    "ES_PASS = os.getenv(\"ES_PASS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global client connection to elastic search\n",
    "es_client = Elasticsearch(\n",
    "    ES_URL,\n",
    "    basic_auth=(ES_USER, ES_PASS),\n",
    "    verify_certs=False,\n",
    "    request_timeout=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'es-sample-es-data-master-2', 'cluster_name': 'es-sample', 'cluster_uuid': 'lxgst327RICarIi1P0c6TQ', 'version': {'number': '8.12.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '1665f706fd9354802c02146c1e6b5c0fbcddfbc9', 'build_date': '2024-01-11T10:05:27.953830042Z', 'build_snapshot': False, 'lucene_version': '9.9.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "print(es_client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indexing BIASQ\n",
    "index_name = \"nq_bm25_metadata\"\n",
    "index_mapping = {\n",
    "    \"settings\" :{\n",
    "    \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"refresh_interval\": \"1m\",\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"possessive_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"possessive_english\"\n",
    "                },\n",
    "                \"light_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"light_english\"\n",
    "                },\n",
    "                \"english_stop\": {\n",
    "                    \"ignore_case\": \"true\",\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": [\"a\", \"about\", \"all\", \"also\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "                                  \"be\", \"been\", \"but\", \"by\", \"can\", \"de\", \"did\", \"do\", \"does\", \"for\", \"from\",\n",
    "                                  \"had\", \"has\", \"have\", \"he\", \"her\", \"him\", \"his\", \"how\", \"if\", \"in\", \"into\",\n",
    "                                  \"is\", \"it\", \"its\", \"more\", \"my\", \"nbsp\", \"new\", \"no\", \"non\", \"not\", \"of\",\n",
    "                                  \"on\", \"one\", \"or\", \"other\", \"our\", \"she\", \"so\", \"some\", \"such\", \"than\",\n",
    "                                  \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\",\n",
    "                                  \"thus\", \"to\", \"up\", \"us\", \"use\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\",\n",
    "                                  \"which\", \"while\", \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"text_en_no_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"text_en_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"english_stop\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"whitespace_lowercase\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"normalizer\": {\n",
    "                \"keyword_lowercase\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"text\"},\n",
    "            \"nqid\": {\"type\": \"text\"},\n",
    "            \"title\":{\"type\": \"text\"},\n",
    "            \"context\": {\"type\": \"text\"},\n",
    "            \"metadata\": {\"type\": \"text\"},\n",
    "            \"keybert_title\":{\"type\": \"text\"},\n",
    "            \"yake_key_idea\":{\"type\": \"text\"},\n",
    "            \"extracted_entities\":{\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name,mapping):\n",
    "    try:\n",
    "        es_client.indices.create(index=index_name,body = mapping)\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    except RequestError as e:\n",
    "        if e.error == 'resource_already_exists_exception':\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "        else:\n",
    "            print(f\"An error occurred while creating index '{index_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'nq_bm25_metadata' created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_index(index_name,index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'nq_knn_metadata' created successfully.\n"
     ]
    }
   ],
   "source": [
    "index_name_knn = 'nq_knn_metadata'\n",
    "index_mapping = {\n",
    "    \"settings\" :{\n",
    "    \"number_of_replicas\": 0,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"refresh_interval\": \"1m\",\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"possessive_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"possessive_english\"\n",
    "                },\n",
    "                \"light_english_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"language\": \"light_english\"\n",
    "                },\n",
    "                \"english_stop\": {\n",
    "                    \"ignore_case\": \"true\",\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": [\"a\", \"about\", \"all\", \"also\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "                                  \"be\", \"been\", \"but\", \"by\", \"can\", \"de\", \"did\", \"do\", \"does\", \"for\", \"from\",\n",
    "                                  \"had\", \"has\", \"have\", \"he\", \"her\", \"him\", \"his\", \"how\", \"if\", \"in\", \"into\",\n",
    "                                  \"is\", \"it\", \"its\", \"more\", \"my\", \"nbsp\", \"new\", \"no\", \"non\", \"not\", \"of\",\n",
    "                                  \"on\", \"one\", \"or\", \"other\", \"our\", \"she\", \"so\", \"some\", \"such\", \"than\",\n",
    "                                  \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\",\n",
    "                                  \"thus\", \"to\", \"up\", \"us\", \"use\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\",\n",
    "                                  \"which\", \"while\", \"why\", \"will\", \"with\", \"would\", \"you\", \"your\", \"yours\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"text_en_no_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"text_en_stop\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"possessive_english_stemmer\",\n",
    "                        \"english_stop\",\n",
    "                        \"light_english_stemmer\"\n",
    "                    ],\n",
    "                    \"tokenizer\": \"standard\"\n",
    "                },\n",
    "                \"whitespace_lowercase\": {\n",
    "                    \"tokenizer\": \"whitespace\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"normalizer\": {\n",
    "                \"keyword_lowercase\": {\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"id\": {\"type\": \"text\"},\n",
    "            \"nqid\": {\"type\": \"text\"},\n",
    "            \"title\":{\"type\": \"text\"},\n",
    "            \"context\": {\"type\": \"text\"},\n",
    "            \"metadata\": {\"type\": \"text\"},\n",
    "            \"keybert_title\":{\"type\": \"text\"},\n",
    "            \"yake_key_idea\":{\"type\": \"text\"},\n",
    "            \"extracted_entities\":{\"type\": \"text\"},\n",
    "            \"contexts_embedding\": {\n",
    "                    \"type\": \"dense_vector\", \"dims\": 384,\n",
    "                    \"similarity\": \"cosine\", \"index\": \"true\"\n",
    "                }\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "create_index(index_name_knn,index_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import ast\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'title', 'text', 'metadata', 'keybert_title', 'yake_key_idea',\n",
       "       'extracted_entities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc =[]\n",
    "index_doc_knn =[]\n",
    "for index, row in merged_df.iterrows():\n",
    "    print('index.....',index)\n",
    "    nq_id = row['_id']\n",
    "    id = \"nq\"+ str(nq_id)\n",
    "    title = row['title']\n",
    "    context =row['text']\n",
    "    metadata =row['metadata']\n",
    "    keybert_title = row['keybert_title']\n",
    "    yake_key_idea = row['yake_key_idea']\n",
    "    extracted_entities = row['extracted_entities']\n",
    "    print(context)\n",
    "    passage_embedding = model.encode(context)\n",
    "    #print(passage_embedding)\n",
    "    doc ={\n",
    "            \"id\": \"\"+id+\"\",\n",
    "            \"nqid\":nq_id,\n",
    "            \"title\": title,\n",
    "            \"context\":context,\n",
    "            \"metadata\":metadata,\n",
    "            \"keybert_title\":keybert_title,\n",
    "            \"yake_key_idea\":yake_key_idea,\n",
    "            \"extracted_entities\":extracted_entities\n",
    "            }\n",
    "    doc_knn ={\n",
    "            \"id\": \"\"+id+\"\",\n",
    "            \"nqid\":nq_id,\n",
    "            \"title\": title,\n",
    "            \"context\":context,\n",
    "            \"metadata\":metadata,\n",
    "            \"keybert_title\":keybert_title,\n",
    "            \"yake_key_idea\":yake_key_idea,\n",
    "            \"extracted_entities\":extracted_entities,\n",
    "            \"contexts_embedding\": passage_embedding\n",
    "\n",
    "            }\n",
    "    index_doc.append(doc)\n",
    "    index_doc_knn.append(doc_knn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62249"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62249"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_doc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'p21645374',\n",
       " 'pubid': 21645374,\n",
       " 'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.',\n",
       "  'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'],\n",
       " 'meshes': ['Alismataceae',\n",
       "  'Apoptosis',\n",
       "  'Cell Differentiation',\n",
       "  'Mitochondria',\n",
       "  'Plant Leaves'],\n",
       " 'labels': ['BACKGROUND', 'RESULTS'],\n",
       " 'long_answer': 'Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant.',\n",
       " 'keybert_title': \"['mitochondria pcd', 'mitochondrial stages']\",\n",
       " 'yake_key_idea': \"['Programmed cell death', 'developmentally regulated PCD']\",\n",
       " 'extracted_entities': \"['Aponogeton', 'A., PCD, TUNEL']\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "documents = []\n",
    "for doc in index_doc:\n",
    "    documents.append(\n",
    "        {\n",
    "            \"_index\": index_name, ## CHANGE INDEX NAME\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers,exceptions, RequestError\n",
    "def chunk_documents(documents, num_chunks):\n",
    "    chunk_size = len(documents) // num_chunks\n",
    "    remainder = len(documents) % num_chunks\n",
    "\n",
    "    start = 0\n",
    "    for i in range(num_chunks):\n",
    "        chunk_end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        yield documents[start:chunk_end]\n",
    "        start = chunk_end\n",
    "\n",
    "# Example usage\n",
    "total_docs = len(documents)\n",
    "num_chunks = 50\n",
    "\n",
    "start_time = time.time()\n",
    "for i, chunk in enumerate(chunk_documents(documents, num_chunks)):\n",
    "    #clear_output(wait=True)\n",
    "    print(f\"Chunk {i+1}: {len(chunk)} documents\")\n",
    "    try:\n",
    "        helpers.bulk(es_client, chunk)\n",
    "        print(\"Done indexing documents into \",{index_name}, \"index!\",{len(chunk)}) ## CHANGE INDEX NAME\n",
    "    except Exception as e: \n",
    "        # Handle the exception\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "documents = []\n",
    "for doc in index_doc_knn:\n",
    "    documents.append(\n",
    "        {\n",
    "            \"_index\": index_name_knn, ## CHANGE INDEX NAME\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_doc_knn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "total_docs = len(documents)\n",
    "num_chunks = 50\n",
    "start_time = time.time()\n",
    "for i, chunk in enumerate(chunk_documents(documents, num_chunks)):\n",
    "    #clear_output(wait=True)\n",
    "    print(f\"Chunk {i+1}: {len(chunk)} documents\")\n",
    "    try:\n",
    "        helpers.bulk(es_client, chunk)\n",
    "        print(\"Done indexing documents into \",{index_name_knn}, \"index!\",{len(chunk)}) ## CHANGE INDEX NAME\n",
    "    except Exception as e: \n",
    "        # Handle the exception\n",
    "        print(\"An error occurred:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magma-juniper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
